#!/usr/bin/env python 

from __future__ import division

import sys
sys.path.append("../")

import gc
import logging
from time import time
import cPickle as pickle

import numpy as np

logger = logging.getLogger()


def run_experiment(args):
    from learning.experiment import Experiment
    from learning.training import Trainer
    from learning.termination import EarlyStopping
    from learning.monitor import MonitorLL, DLogModelParams, SampleFromP
    from learning.monitor.bootstrap import BootstrapLL
    from learning.dataset import MNIST
    from learning.preproc import PermuteColumns

    from learning.rws  import LayerStack
    from learning.sbn  import SBN, SBNTop
    from learning.darn import DARN, DARNTop
    from learning.nade import NADE, NADETop

    np.set_printoptions(precision=2)

    logger.debug("Arguments %s" % args)
    tags = []

    np.random.seed(23)

    # Dataset
    if args.shuffle:
        np.random.seed(23)
        preproc = [PermuteColumns()]
        tags += ["shuffle"]
    else:
        preproc = []

    dataset = MNIST(fname="mnist_salakhutdinov.pkl.gz", which_set='train', preproc=preproc, n_datapoints=50000)
    valiset = MNIST(fname="mnist_salakhutdinov.pkl.gz", which_set='valid', preproc=preproc, n_datapoints=10000)
    testset = MNIST(fname="mnist_salakhutdinov.pkl.gz", which_set='test', preproc=preproc, n_datapoints=10000)

    sleep_interleave=2

    # Samples
    n_samples = args.samples
    tags += ["spl%d"%n_samples]

    # Batch size
    batch_size = args.batchsize
    tags += ["bs%d"%batch_size]

    # Lookahead
    if args.lookahead != 10:
        tags += ["lah%d" % args.lookahead]

    # Learning rate
    def lr_tag(value, prefix):
        exp = np.floor(np.log10(value))
        leading = ("%e"%value)[0]
        return ["%s%s%d" % (prefix, leading, -exp)]

    lr_base = args.lr
    tags += lr_tag(lr_base, prefix="lr")
    lr_p = args.lr_p
    lr_q = args.lr_q
    lr_s = args.lr_s
    if lr_p is None:
        lr_p = lr_base
    else:
        tags += lr_tag(lr_p, prefix="lp")
    if lr_q is None:
        lr_q = lr_base
    else:
        tags += lr_tag(lr_q, prefix="lq")
    if lr_s is None:
        lr_s = lr_base
    else:
        tags += lr_tag(lr_s, prefix="ls")
    
    if args.ldiscount != 1.0:
        tags += ["ld"]
    
    # Layer models
    layer_models = {
        "sbn" : (SBN, SBNTop),
        "darn": (DARN, DARNTop), 
        "nade": (NADE, NADETop),
    }

    if not args.p_model in layer_models:
        raise "Unknown P-layer model %s" % args.p_model
    p_layer, p_top = layer_models[args.p_model]

    if not args.q_model in layer_models:
        raise "Unknown P-layer model %s" % args.p_model
    q_layer, q_top = layer_models[args.q_model]

    n_X = 28*28
    p_layers = []
    q_layers = []

    tags.sort()


    experiment = Experiment()
    expname = "grow-%s-%s-%s"% ("-".join(tags), args.p_model, args.q_model)
    experiment.setup_output_dir(expname)
    experiment.setup_logging()

    logger.info("Running %s" % expname)

    def disposable_prior(layer_size):
        p_prior = [p_top(n_X=layer_size, clamp_sigmoid=False)]
        q_prior = []
        return (p_prior, q_prior)

    size_factor = 0.7 
    final_size = 50

    top_size = n_X

    p_layers = []
    q_layers = []
    iteration = 0 
    while top_size > final_size:
        iteration = iteration + 1
        new_top_size = int(size_factor * top_size)

        p_layers.append(
            p_layer(n_X=top_size, n_Y=new_top_size, clamp_sigmoid=True)
        )
        q_layers.append(
            q_layer(n_X=new_top_size, n_Y=top_size, clamp_sigmoid=True)
        )

        p_prior, q_prior = disposable_prior(new_top_size)

        # Assemble model and train woth lower layers fixed
        model = LayerStack(
            loose_prior=True,
            fix_layers=range(len(p_layers)-1),
            p_layers=p_layers+p_prior,
            q_layers=q_layers+q_prior,
        )
        model.setup()

        gc.collect()

        trainer = Trainer(
            batch_size=batch_size,
            n_samples=n_samples,
            sleep_interleave=sleep_interleave,
            learning_rate_p=lr_p,
            learning_rate_q=lr_q,
            learning_rate_s=lr_s,
            layer_discount=args.ldiscount,
            dataset=dataset, 
            model=model,
            termination=EarlyStopping(lookahead=2, min_epochs=10),
            epoch_monitors=[
                DLogModelParams(),
                MonitorLL(name="valiset", data=valiset, n_samples=[1, 5, 25, 100]),
            ],
            final_monitors=[]
        )
        experiment.setup_output_dir("%s-%d-a" % (expname, iteration))
        experiment.setup_logging()
        experiment.trainer = trainer
        experiment.print_summary()
        experiment.run_experiment()

        # ... finetune lower layers
        model = LayerStack(
            loose_prior=True,
            p_layers=p_layers+p_prior,
            q_layers=q_layers+q_prior,
        )
        model.setup()

        gc.collect()

        trainer = Trainer(
            batch_size=batch_size,
            n_samples=n_samples,
            sleep_interleave=sleep_interleave,
            learning_rate_p=lr_p,
            learning_rate_q=lr_q,
            learning_rate_s=lr_s,
            layer_discount=args.ldiscount,
            dataset=dataset, 
            model=model,
            termination=EarlyStopping(lookahead=2, min_epochs=10),
            epoch_monitors=[
                DLogModelParams(),
                MonitorLL(name="valiset", data=valiset, n_samples=[1, 5, 25, 100]),
            ],
            final_monitors=[]
        )
        experiment.setup_output_dir("%s-%d-b" % (expname, iteration))
        experiment.setup_logging()
        experiment.trainer = trainer
        experiment.print_summary()
        experiment.run_experiment()

        top_size = new_top_size

    # And put final prior on top...
    #p_layers.append( p_top(n_X=n_X, clamp_sigmoid=True) )

    logger.info("Finished. Wrinting metadata")
    experiment.print_summary()


#=============================================================================
if __name__ == "__main__":
    import argparse 

    parser = argparse.ArgumentParser()
    parser.add_argument('--verbose', '-v', action='count')
    parser.add_argument('--shuffle', action='store_true', default=False)
    parser.add_argument('--samples', default=10, type=int, 
        help="Number of training samples (default: 10)")
    parser.add_argument('--batchsize', default=50, type=int, 
        help="Mini batch size (default: 25)")
    parser.add_argument('--sleep-interleave', '--si', default=2, type=int, 
        help="Sleep interleave (default: 2)")
    parser.add_argument('--lr', default=1e-3, type=float, help="Learning rate (default: 1e-3)")
    parser.add_argument('--lr_p', default=None, type=float, help="p learning rate")
    parser.add_argument('--lr_q', default=None, type=float, help="wake-q-learing rate")
    parser.add_argument('--lr_s', default=None, type=float, help="sleep-q-learning rate")
    parser.add_argument('--ldiscount', default=1., type=float, help="layer_discount")
    parser.add_argument('--split59k', '--s59k', default=False, action="store_true", 
        help="Use 59k-1k-10k dataset split");
    parser.add_argument('--lookahead', default=10, type=int, 
        help="Termination criteria: # epochs without LL increase")
    parser.add_argument('p_model', default="SBN", 
        help="SBN, DARN or NADE (default: SBN")
    parser.add_argument('q_model', default="SBN",
        help="SBN, DARN or NADE (default: SBN")
    #parser.add_argument('layer_sizes', default="200,200,10", 
    #    help="Comma seperated list of sizes. Layer cosest to the data comes first")
    args = parser.parse_args()

    FORMAT = '[%(asctime)s] %(name)-15s %(message)s'
    DATEFMT = "%H:%M:%S"
    logging.basicConfig(format=FORMAT, datefmt=DATEFMT, level=logging.INFO)

    run_experiment(args)
